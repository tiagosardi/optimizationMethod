{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "metodoNewton.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tiagosardi/optimizationMethod/blob/main/metodoNewton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbuWOQ4ZqyW9"
      },
      "source": [
        "<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n",
        "\n",
        "\n",
        "Método direto para situações em que o valor do gradiente não exista, $$\\nexists\\nabla \\hat{f}(x),$$ vamos estimar o valor do gradiente aplicando: \n",
        "$$\n",
        "\\lim_{delta \\to 0} \\frac{f(x+delta)-f(x)}{delta}\n",
        "$$\n",
        "\n",
        "Onde estimaremos numa aproximação curta do valor real do gradiente. Para isso, criaremos uma função quadrática que possa convergir com o mínimo local da função original.\n",
        "\n",
        "Vamos utilizar o método de Newton. Partiremos da minimização da expansão da série de Taylor. Então, utilizaremos o gradiente igual a zero da função de Taylor:\n",
        "\n",
        "$$min \\hat{f}(x) = f(x_i) + f'(x_i)(x-x_i) + \\frac{f''(x_i)}{2!}(x-x_i)^2$$\n",
        "\n",
        "$$\\hat{f}'(x)=0-f'(x_i)-f''(x_i)(x- x_i) = 0$$\n",
        "\n",
        "$$x = x_i - \\frac{f'(x)}{f''(x)}$$\n",
        "\n",
        "Repare que consigo ir para o mínimo da função quadrática usando um método que estima o gradiente. Se a função for quadrática, conseguiremos convergir em apenas uma iteração.\n",
        "\n",
        "Portanto, para efetuar as etapas deste processo para uma variável, seguimos a seguinte receita:\n",
        "\n",
        "1. Escolho um X inicial\n",
        "2. Escolho a direção da busca $$\\tilde{s}=f'(x)$$\n",
        "3. Atualiza x $$x=x-\\frac{1}{f(\\tilde{x})}f'(x)$$\n",
        "4. Ir para 2 até atingir critério de parada\n",
        "\n",
        "Para várias variáveis, resolveremos utilizando a Hessiana:\n",
        "\n",
        "$$\\tilde{x} = \\tilde{x} - H^{-1} \\nabla{f}(\\tilde{x})$$\n",
        "\n",
        "Utilizamos a inversa da Hessiana, pois é a maneira algebricamente correta de calcular a ração do vetor gradiente com a hessiana. \n",
        "Temos o problema da Hessiana ser singular (determinante 0).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLrYqPaf-vj5",
        "outputId": "d5ae60ae-db5d-468d-aaac-077dd18f6e5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def newton_method(x0,f,grad,H,tol=1e-6):\n",
        "    \n",
        "    x = np.array(x0)\n",
        "\n",
        "    list_x = [x]\n",
        "    list_fx = [f(x)]\n",
        "\n",
        "    while np.linalg.norm(grad(x)) > tol:\n",
        "        \n",
        "        hinv = np.linalg.inv(H(x))\n",
        "        g = np.array([grad(x)]).T\n",
        "        delta = np.matmul(hinv,g)\n",
        "        x = x - delta.T\n",
        "        x = x[0]\n",
        "        \n",
        "        list_x.append(x)\n",
        "        list_fx.append(f(x))\n",
        "\n",
        "        g = grad(x)\n",
        "\n",
        "    return x,f(x),list_x,list_fx\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  \n",
        "\n",
        "    f = lambda x: (100*(x[1] - x[0]**2)**2 + (x[0]-1)**2) \n",
        "    grad = lambda x: [-400*(x[1]-x[0]**2)*x[0] + 2*(x[0]-1), 200*(x[1]-x[0]**2)]\n",
        "    H = lambda x: [[-400*x[1]+1200*x[0]**2,-400*x[0]],[-400*x[0],200]]\n",
        "    xs,fs,xlist,fx_list = newton_method([1,-2],f,grad,H)\n",
        "\n",
        "    print(xs)\n",
        "    print(xlist)\n",
        "    "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1. 1.]\n",
            "[array([ 1, -2]), array([1., 1.])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFJ7mhMCBNmr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}